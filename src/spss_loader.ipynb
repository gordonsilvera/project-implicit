{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPSS file loader\n",
    "\n",
    "Loads large SPSS files to BigQuery.\n",
    "\n",
    "__To Do__\n",
    "- Update schema with YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "from savReaderWriter import SavReader\n",
    "from database_connection import databaseConnection\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "\n",
    "def get_db_client(full_table_id):\n",
    "    db = databaseConnection()\n",
    "    project_id, dataset_id, table_id = _get_table_id_set(full_table_id)\n",
    "    client = bigquery.Client(\n",
    "        project=project_id,\n",
    "        credentials=db.credentials)\n",
    "    return client\n",
    "\n",
    "\n",
    "def delete_table(full_table_id):\n",
    "    # If the table does not exist, delete_table raises\n",
    "    # google.api_core.exceptions.NotFound unless not_found_ok is True.\n",
    "    client = get_db_client(full_table_id)\n",
    "    client.delete_table(full_table_id, not_found_ok=True)  # Make an API request.\n",
    "    print(\"Deleted table '{}'.\".format(full_table_id))\n",
    "\n",
    "\n",
    "def _get_table_id_set(full_table_id):\n",
    "    project_id = full_table_id.split('.')[0]\n",
    "    dataset_id = full_table_id.split('.')[1]\n",
    "    table_id = full_table_id.split('.')[2]\n",
    "    return (project_id, dataset_id, table_id)\n",
    "\n",
    "\n",
    "def _get_chunck_cutoffs(file_length, interval=50_000):\n",
    "    \"\"\" Create a list of lists containing cutoff levels\n",
    "    to chunk SPSS files.\n",
    "    \"\"\"\n",
    "    i0=0\n",
    "    i1=interval\n",
    "    c_list=[]\n",
    "    while i1 <= file_length:\n",
    "        c = [i0, i1]\n",
    "        c_list += [c]\n",
    "        i0 = i1 + 1\n",
    "        i1 = i0 + (interval - 1)\n",
    "    c_list = c_list + [[i0, file_length]]\n",
    "    return c_list\n",
    "\n",
    "\n",
    "def spss_to_csv(filename):\n",
    "    with SavReader(filename) as reader:\n",
    "        header = [re.sub(\"(\\'|b)\", \"\", str(h)) for h in reader.header]\n",
    "        file_length = len(reader)\n",
    "        df = pd.DataFrame(columns=header)\n",
    "        chunck_cutoffs = _get_chunck_cutoffs(file_length)\n",
    "        file_list = []\n",
    "        for i, chunk in enumerate(chunck_cutoffs):\n",
    "            lines = []\n",
    "            for line in reader[chunk[0] : chunk[1]]:\n",
    "                lines += [line]\n",
    "            df_tmp = pd.DataFrame(lines, columns=header)\n",
    "            filename_out = re.sub(\"\\.sav\", \"_{}.csv\".format(i), filename)\n",
    "            file_list += [filename_out]\n",
    "            print(\"LOADING {} of {}: (lines {} of {}) --> {}\".format(\n",
    "                i+1, len(chunck_cutoffs), chunk[1], file_length, filename_out))\n",
    "            df_tmp.to_csv(filename_out)\n",
    "            del lines, df_tmp\n",
    "        return file_list\n",
    "\n",
    "\n",
    "def csv_to_db(source_file, full_table_id, replace=True):\n",
    "    \"\"\" Load a CSV file to BigQuery using the BigQuery Python API. \n",
    "    \n",
    "    Example:\n",
    "        ```\n",
    "        csv_to_db(\n",
    "            file = ,\n",
    "            full_table_id = )\n",
    "        ```\n",
    "    \n",
    "    Attributes:\n",
    "        source_file (str): The file path/name of the CSV file. Include\n",
    "            the file suffix (i.e. .csv).\n",
    "            \n",
    "        full_table_id (str): BigQuery table ID. Example:\n",
    "            `my-project.dope_dataset.terrific_table`\n",
    "            \n",
    "        replace (boolean): Whether to replace the table, if it exists.\n",
    "    \n",
    "    To Do:\n",
    "        * Load table schema from YAML\n",
    "    \n",
    "    \"\"\"\n",
    "    project_id, dataset_id, table_id = _get_table_id_set(full_table_id)\n",
    "    client = get_db_client(full_table_id)\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.skip_leading_rows = 1\n",
    "    job_config.autodetect = True\n",
    "    if replace:\n",
    "        delete_table(full_table_id)\n",
    "    with open(source_file, \"rb\") as sf:\n",
    "        job = client.load_table_from_file(sf, table_ref, job_config=job_config)\n",
    "    job.result()  # Waits for table load to complete.\n",
    "    print(\"Loaded {} rows into {}:{}.{}.\".format(\n",
    "        job.output_rows, project_id, dataset_id, table_id))\n",
    "\n",
    "\n",
    "def spss_to_db(filename, full_table_id, replace=True):\n",
    "    \"\"\" Load SPSS files to BigQuery using the BigQuery Python API. \n",
    "    \n",
    "    This function breaks large SPSS files into smaller CSVs and \n",
    "    then loads them into BigQuery.\n",
    "    \n",
    "    Example:\n",
    "        ```\n",
    "        spss_to_db(\n",
    "            filename = 'dopeData.sav',\n",
    "            full_table_id = 'my-project.dope_data.awesome_table')\n",
    "        ```\n",
    "    \n",
    "    Attributes:\n",
    "        filename (str): The file path/name of the CSV file. Include\n",
    "            the file suffix (i.e. .csv).\n",
    "            \n",
    "        full_table_id (str): BigQuery table ID. Example:\n",
    "            `my-project.dope_dataset.terrific_table`\n",
    "            \n",
    "        replace (boolean): Whether to replace the table, if it exists.\n",
    "    \"\"\"\n",
    "    project_id, dataset_id, table_id = _get_table_id_set(full_table_id)\n",
    "    client = get_db_client(full_table_id)\n",
    "    print(\"STEP 1: CONVERT SPSS FILES INTO CSV FILE(S)\\n\")\n",
    "    file_list = list(set(spss_to_csv(filename)))\n",
    "    print(\"\\nSTEP 2: LOAD CSV FILE(S) INTO BIGQUERY TABLE\\n\")\n",
    "    if replace:\n",
    "        delete_table(full_table_id)\n",
    "    for i, file in enumerate(file_list):\n",
    "        print(\"LOADING {} of {}: {} --> {}:{}.{}.\".format(\n",
    "            i+1, len(file_list), file, project_id, dataset_id, table_id))\n",
    "        csv_to_db(file, full_table_id, replace=False)\n",
    "    print(\"\\nSPSS FILE LOAD COMPLETE!\\n\\n\"\n",
    "          \"Note: you have a number of CSV files in your data folder.\"\n",
    "          \"You should delete them before merging to a repository.\"\n",
    "         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Run spss_to_db\n",
    "\n",
    "`spss_to_db()` breaks large SPSS files into smaller CSVs and then loads them into BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file to BigQuery\n",
    "# csv_file = '/home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_0.csv'\n",
    "# full_table_id = 'algomosaic-nyc.project_implicit.race_ait_2019'\n",
    "# csv_to_db(source_file=filename, full_table_id=full_table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a SPSS file to BigQuery\n",
    "spss_file = '/home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019.sav'\n",
    "full_table_id = 'algomosaic-nyc.project_implicit.race_ait_2019'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: CONVERT SPSS FILES INTO CSV FILE(S)\n",
      "\n",
      "LOADING 0 of 18: (lines 50000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_0.csv\n",
      "LOADING 1 of 18: (lines 100000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_1.csv\n",
      "LOADING 2 of 18: (lines 150000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_2.csv\n",
      "LOADING 3 of 18: (lines 200000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_3.csv\n",
      "LOADING 4 of 18: (lines 250000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_4.csv\n",
      "LOADING 5 of 18: (lines 300000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_5.csv\n",
      "LOADING 6 of 18: (lines 350000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_6.csv\n",
      "LOADING 7 of 18: (lines 400000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_7.csv\n",
      "LOADING 8 of 18: (lines 450000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_8.csv\n",
      "LOADING 9 of 18: (lines 500000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_9.csv\n",
      "LOADING 10 of 18: (lines 550000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_10.csv\n",
      "LOADING 11 of 18: (lines 600000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_11.csv\n",
      "LOADING 12 of 18: (lines 650000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_12.csv\n",
      "LOADING 13 of 18: (lines 700000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_13.csv\n",
      "LOADING 14 of 18: (lines 750000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_14.csv\n",
      "LOADING 15 of 18: (lines 800000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_15.csv\n",
      "LOADING 16 of 18: (lines 850000 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_16.csv\n",
      "LOADING 17 of 18: (lines 875209 of 875209), /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_17.csv\n",
      "\n",
      "STEP 2: LOAD CSV FILE(S) INTO BIGQUERY TABLE\n",
      "\n",
      "Deleted table 'algomosaic-nyc.project_implicit.race_ait_2019'.\n",
      "LOADING 0 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_9.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 1 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_12.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 2 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_16.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 3 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_2.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 4 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_5.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 5 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_11.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 6 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_13.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 7 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_6.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 8 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_4.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 9 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_3.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 10 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_17.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 25208 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 11 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_0.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 50000 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 12 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_7.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 13 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_8.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 14 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_15.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 15 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_10.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 16 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_14.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "LOADING 17 of 18: /home/jovyan/project-implicit/data/race_iat/Race_IAT.public.2019_1.csv into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "Loaded 49999 rows into algomosaic-nyc:project_implicit.race_ait_2019.\n",
      "\n",
      "SPSS FILE LOAD COMPLETE!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spss_to_db(spss_file, full_table_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the table\n",
    "#                  to add an empty column.\n",
    "full_table_id = \"your-project.your_dataset.your_table_name\"\n",
    "\n",
    "table = client.get_table(full_table_id)  # Make an API request.\n",
    "\n",
    "original_schema = table.schema\n",
    "new_schema = original_schema[:]  # Creates a copy of the schema.\n",
    "new_schema.append(bigquery.SchemaField(\"phone\", \"STRING\"))\n",
    "\n",
    "table.schema = new_schema\n",
    "table = client.update_table(table, [\"schema\"])  # Make an API request.\n",
    "\n",
    "if len(table.schema) == len(original_schema) + 1 == len(new_schema):\n",
    "    print(\"A new column has been added.\")\n",
    "else:\n",
    "    print(\"The column has not been added.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
